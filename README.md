# LLM-with-RL-papers
A collection of LLM with RL related papers

## RL without Human Feedback

[1] Le H, Wang Y, Gotmare A D, et al. **Coderl: Mastering code generation through pretrained models and deep reinforcement learning**[J]. Advances in Neural Information Processing Systems, 2022, 35: 21314-21328. [[link]](https://proceedings.neurips.cc/paper_files/paper/2022/file/8636419dea1aa9fbd25fc4248e702da4-Paper-Conference.pdf)

[2] Shojaee P, Jain A, Tipirneni S, et al. **Execution-based Code Generation using Deep Reinforcement Learning**[J]. arXiv preprint arXiv:2301.13816, 2023.[[link]](https://arxiv.org/pdf/2301.13816)

[3] Uesato J, Kushman N, Kumar R, et al. **Solving math word problems with process-and outcome-based feedback**[J]. arXiv preprint arXiv:2211.14275, 2022.[[link]](https://arxiv.org/pdf/2211.14275)

[4] Zhang T, Liu F, Wong J, et al. **The Wisdom of Hindsight Makes Language Models Better Instruction Followers**[J]. arXiv preprint arXiv:2302.05206, 2023.[[link]](https://arxiv.org/pdf/2302.05206.pdf)

## RLHF(RL with Human Feedback)

[1] Ouyang L, Wu J, Jiang X, et al. **Training language models to follow instructions with human feedback**[J]. Advances in Neural Information Processing Systems, 2022, 35: 27730-27744.[[link]](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)

[2] Nakano R, Hilton J, Balaji S, et al. **Webgpt: Browser-assisted question-answering with human feedback**[J]. arXiv preprint arXiv:2112.09332, 2021.[[link]](https://arxiv.org/pdf/2112.09332)

[3] Bai Y, Kadavath S, Kundu S, et al. **Constitutional AI: Harmlessness from AI Feedback**[J]. arXiv preprint arXiv:2212.08073, 2022.[[link]](https://arxiv.org/pdf/2212.08073)





