# LLM-with-RL-papers
A collection of LLM with RL related papers for instruction following, reasoning, decision making, continuous improvement and self improvement etc.

## Review

[1] Yang S, Nachum O, Du Y, et al. **Foundation Models for Decision Making: Problems, Methods, and Opportunities**[J]. arXiv preprint arXiv:2303.04129, 2023.[[link]](https://arxiv.org/pdf/2303.04129)

## RL without Human Feedback

[1] Le H, Wang Y, Gotmare A D, et al. **Coderl: Mastering code generation through pretrained models and deep reinforcement learning**[J]. Advances in Neural Information Processing Systems, 2022, 35: 21314-21328. [[link]](https://proceedings.neurips.cc/paper_files/paper/2022/file/8636419dea1aa9fbd25fc4248e702da4-Paper-Conference.pdf)

[2] Shojaee P, Jain A, Tipirneni S, et al. **Execution-based Code Generation using Deep Reinforcement Learning**[J]. arXiv preprint arXiv:2301.13816, 2023.[[link]](https://arxiv.org/pdf/2301.13816)

[3] Uesato J, Kushman N, Kumar R, et al. **Solving math word problems with process-and outcome-based feedback**[J]. arXiv preprint arXiv:2211.14275, 2022.[[link]](https://arxiv.org/pdf/2211.14275)


## RLHF(RL with Human Feedback)

[1] Christiano P F, Leike J, Brown T, et al. **Deep reinforcement learning from human preferences**[J]. Advances in neural information processing systems, 2017, 30. [[link]](https://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf)

[2] Ziegler D M, Stiennon N, Wu J, et al. **Fine-tuning language models from human preferences**[J]. arXiv preprint arXiv:1909.08593, 2019.[[link]](https://arxiv.org/pdf/1909.08593.pdf))

[3] Ouyang L, Wu J, Jiang X, et al. **Training language models to follow instructions with human feedback**[J]. Advances in Neural Information Processing Systems, 2022, 35: 27730-27744.[[link]](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)

[4] Nakano R, Hilton J, Balaji S, et al. **Webgpt: Browser-assisted question-answering with human feedback**[J]. arXiv preprint arXiv:2112.09332, 2021.[[link]](https://arxiv.org/pdf/2112.09332)

[5] Bai Y, Kadavath S, Kundu S, et al. **Constitutional AI: Harmlessness from AI Feedback**[J]. arXiv preprint arXiv:2212.08073, 2022.[[link]](https://arxiv.org/pdf/2212.08073)

### Prompt-based but RL related 

[1] Madaan A, Tandon N, Gupta P, et al. **Self-refine: Iterative refinement with self-feedback**[J]. arXiv preprint arXiv:2303.17651, 2023.[[link]](https://arxiv.org/pdf/2303.17651)

[2] Yao S, Zhao J, Yu D, et al. **React: Synergizing reasoning and acting in language models**[J]. arXiv preprint arXiv:2210.03629, 2022.[[link]](https://arxiv.org/pdf/2210.03629)

[3] Liu H, Sferrazza C, Abbeel P. **Languages are rewards: Hindsight finetuning using human feedback**[J]. arXiv preprint arXiv:2302.02676, 2023.[[link]](https://arxiv.org/pdf/2302.02676)

[4] Zhang T, Liu F, Wong J, et al. **The Wisdom of Hindsight Makes Language Models Better Instruction Followers**[J]. arXiv preprint arXiv:2302.05206, 2023.[[link]](https://arxiv.org/pdf/2302.05206.pdf)

[5] Chen X, Lin M, Sch√§rli N, et al. **Teaching Large Language Models to Self-Debug**[J]. arXiv preprint arXiv:2304.05128, 2023.[[link]](https://arxiv.org/pdf/2304.05128)








